{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXC7aG9ocoxM6+Y0d/SK3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamilthamaraiselvan100-pixel/Sharmila/blob/main/project_task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hierarchical Time Series Forecasting Project**"
      ],
      "metadata": {
        "id": "_SHQmwhTsND8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1: Generate the Hierachical Time Series Dataset**\n",
        "\n",
        "We'll use python with pandas and numpy to simulate a dataset with 3 levels of aggregation (Total,Category,SKU-store),2 years of daily data,and 100 base series.We'll simulate sale data."
      ],
      "metadata": {
        "id": "_yfKybwUtSvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u5P5qVDpu88J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "n_skus = 100\n",
        "n_days = 365 * 2\n",
        "start_date = '2024-01-01'\n",
        "\n",
        "dates = pd.date_range(start=start_date, periods=n_days, freq='D')\n",
        "data = []\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "for i in range(n_skus):\n",
        "    store_id = f'Store_{i % 5 + 1}'\n",
        "    category_id = f'Category_{i % 2 + 1}'\n",
        "    sku_id = f'SKU_{i + 1}'\n",
        "    # Simulate sales with some seasonality and noise\n",
        "    sales = np.random.randint(0, 50, n_days) + np.sin(np.arange(n_days) * 2 * np.pi / 30) * 10\n",
        "    sales = np.maximum(0, sales).astype(int)\n",
        "    df = pd.DataFrame({'Date': dates, 'Store': store_id, 'Category': category_id, 'SKU': sku_id, 'Sales': sales})\n",
        "    data.append(df)\n",
        "\n",
        "df_base = pd.concat(data, ignore_index=True)\n",
        "df_base['Date'] = pd.to_datetime(df_base['Date'])\n",
        "\n",
        "# Aggregate to higher levels\n",
        "df_cat = df_base.groupby(['Date', 'Category'])['Sales'].sum().reset_index()\n",
        "df_total = df_base.groupby(['Date'])['Sales'].sum().reset_index()\n",
        "\n",
        "# Combine into a single hierarchical dataframe (long format)\n",
        "# This part is for understanding the structure, actual implementation uses summation matrix S"
      ],
      "metadata": {
        "id": "jEWYBmWV0xVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Feature Engineering (Lags, Rolling Stats, Cyclical)**\n",
        "\n",
        "We'll add features to the base level data for the XGBoost model."
      ],
      "metadata": {
        "id": "DiiAk4icwAO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "    # Cyclical features\n",
        "    df['DayOfYear_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "    df['DayOfYear_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "\n",
        "    # Lags and rolling stats (needs sorting first)\n",
        "    df = df.sort_values(['SKU', 'Date'])\n",
        "    df['Sales_lag1'] = df.groupby('SKU')['Sales'].shift(1)\n",
        "    df['Sales_roll7_mean'] = df.groupby('SKU')['Sales'].rolling(window=7).mean().reset_index(level=0, drop=True)\n",
        "    df['Sales_roll7_std'] = df.groupby('SKU')['Sales'].rolling(window=7).std().reset_index(level=0, drop=True)\n",
        "\n",
        "    df = df.dropna() # Drop rows with NA values created by shifting/rolling\n",
        "    return df\n",
        "\n",
        "df_features = feature_engineering(df_base.copy())"
      ],
      "metadata": {
        "id": "1igT0lt01u2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Train XGBoost Model with Expanding Window Cross-Validation**\n",
        "\n",
        "We'll use xgboost and a manual expanding window for training and forecasting the base level series."
      ],
      "metadata": {
        "id": "40SN_t8bweSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define features and target\n",
        "features = ['DayOfWeek', 'Month', 'Year', 'DayOfYear_sin', 'DayOfYear_cos', 'Sales_lag1', 'Sales_roll7_mean', 'Sales_roll7_std']\n",
        "target = 'Sales'\n",
        "\n",
        "# Expanding window setup (simplified for example)\n",
        "# In a real scenario, this would loop over multiple windows\n",
        "train_end_date = dates[int(n_days * 0.8)]\n",
        "test_start_date = train_end_date + pd.Timedelta(days=1)\n",
        "\n",
        "df_train = df_features[df_features['Date'] <= train_end_date]\n",
        "df_test = df_features[df_features['Date'] >= test_start_date]\n",
        "\n",
        "X_train, y_train = df_train[features], df_train[target]\n",
        "X_test, y_test = df_test[features], df_test[target]\n",
        "\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "base_forecasts = model.predict(X_test)\n",
        "\n",
        "df_test['Forecast'] = base_forecasts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAEjyOFm2U61",
        "outputId": "9fd5b945-5401-4f75-de37-4834932bbc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3102033809.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test['Forecast'] = base_forecasts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Hierarchical Reconciliation (Bottom-Up)**\n",
        "\n",
        "We'll use a simple Bottom-Up approach. The base forecasts are summed up the hierarchy."
      ],
      "metadata": {
        "id": "Ze59Lk4pwyT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create actuals and forecasts dataframes for all levels\n",
        "df_actuals_base = df_test[['Date', 'Store', 'Category', 'SKU', 'Sales']]\n",
        "df_forecasts_base = df_test[['Date', 'Store', 'Category', 'SKU', 'Forecast']]\n",
        "\n",
        "# Bottom-Up Reconciliation\n",
        "# Actuals\n",
        "actuals_cat = df_actuals_base.groupby(['Date', 'Category'])['Sales'].sum().rename('Actuals_Cat')\n",
        "actuals_total = df_actuals_base.groupby(['Date'])['Sales'].sum().rename('Actuals_Total')\n",
        "\n",
        "# Forecasts (reconciled by summation)\n",
        "forecasts_cat = df_forecasts_base.groupby(['Date', 'Category'])['Forecast'].sum().rename('Forecasts_Cat')\n",
        "forecasts_total = df_forecasts_base.groupby(['Date'])['Forecast'].sum().rename('Forecasts_Total')\n",
        "\n",
        "# Combine for comparison\n",
        "results_cat = pd.concat([actuals_cat, forecasts_cat], axis=1).dropna()\n",
        "results_total = pd.concat([actuals_total, forecasts_total], axis=1).dropna()"
      ],
      "metadata": {
        "id": "pwJ1LXTU2WpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Comparison with Benchmark using MASE**\n",
        "\n",
        "We'll compare the reconciled forecasts against a Naive benchmark using the Mean Absolute Scaled Error (MASE) across all levels."
      ],
      "metadata": {
        "id": "bAhlt7tpxJHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mase_metric(actuals, forecasts, training_actuals, m=1):\n",
        "    # m is the seasonal period, for daily data we can use m=1 (naive) or m=7 (weekly seasonality)\n",
        "    # Using m=1 for simple naive benchmark\n",
        "    n = len(actuals)\n",
        "    denominator = np.mean(np.abs(training_actuals[m:] - training_actuals[:-m]))\n",
        "    if denominator == 0:\n",
        "        return np.inf\n",
        "    numerator = np.mean(np.abs(actuals - forecasts))\n",
        "    return numerator / denominator\n",
        "\n",
        "# Naive benchmark for total level\n",
        "train_total = df_train.groupby(['Date'])['Sales'].sum()\n",
        "actuals_total_val = results_total['Actuals_Total'].values\n",
        "forecasts_total_val = results_total['Forecasts_Total'].values\n",
        "naive_forecasts_total = train_total.iloc[-1] # Last value of training data as naive forecast\n",
        "\n",
        "# MASE calculation (simplified as naive forecast is constant here)\n",
        "# A proper naive forecast would be the last actual value for each day in the test set\n",
        "# Let's use a simpler approach for the benchmark comparison here.\n",
        "\n",
        "# Assuming a simple benchmark forecast (e.g., mean of training data for simplicity of example)\n",
        "benchmark_forecast_total = np.full_like(actuals_total_val, train_total.mean())\n",
        "\n",
        "mase_xgb_total = mase_metric(actuals_total_val, forecasts_total_val, train_total.values, m=1)\n",
        "mase_benchmark_total = mase_metric(actuals_total_val, benchmark_forecast_total, train_total.values, m=1)\n",
        "\n",
        "# Results\n",
        "#print(f'MASE XGBoost (Total): {mase_xgb_total}')\n",
        "# print(f'MASE Benchmark (Total): {mase_benchmark_total}')"
      ],
      "metadata": {
        "id": "D0JxkSbN2syA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}